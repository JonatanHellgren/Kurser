{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the perceptron learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to implement the perceptron learning algorithm using NumPy. We implement the methods `fit` and `predict` so that our classifier can be used in the same way as any scikit-learn classifier. (See the [scikit-learn documentation](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html).)\n",
    "\n",
    "The code uses a little bit of object-oriented programming. It's not anything particularly complicated, but if you're not used to object-oriented programming in Python, you might take a look at [this tutorial](https://python.swaroopch.com/oop.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a class that represents linear classifiers in general. This class does not have a `fit` method, because that will be implemented by subclasses representing specific learning algorithms for linear classifiers, e.g. the perceptron. So the thing we need to do here is to implement the `predict` method, because prediction works identically for all linear classifiers, regardless of how they were trained.\n",
    "\n",
    "We also include a helper method `find_classes`, which finds the two output classes and associates them with positive and negative classifier scores, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    \n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of classes\n",
    "        is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[0]\n",
    "        self.negative_class = classes[1]\n",
    "    \n",
    "    def predict(self, X):        \n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be stored in\n",
    "        a matrix, where each row contains the features for one instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = X.dot(self.w)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores>=0.0, scores<0.0], \n",
    "                        [self.positive_class, \n",
    "                         self.negative_class])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the class that implements the perceptron learning algorithm. The actual learning algorithm is in the method called `fit`.\n",
    "\n",
    "Note that this class has the same name as the `Perceptron` class in scikit-learn, so be careful when you import so that you don't get a name clash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(LinearClassifier):\n",
    "    \n",
    "    def __init__(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how \n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros( n_features )\n",
    "\n",
    "        for i in range(self.n_iter):            \n",
    "            for x, y in zip(X, Y):\n",
    "                \n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)    \n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y == self.positive_class and score <= 0:\n",
    "                    self.w += x\n",
    "                if y == self.negative_class and score >= 0:\n",
    "                    self.w -= x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our perceptron implementation on the Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test our perceptron implementation on the Adult dataset. This example reuses some code from the first computer exercise, to process the format of the dataset. To use this dataset, you need to download the files `adult.names`, `adult.data`, and `adult.test` from [the UCI machine learning repository](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(filename):\n",
    "    names = []\n",
    "    types = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            if l[0] == '|' or ':' not in l:\n",
    "                continue\n",
    "            cols = l.split(':')\n",
    "            names.append(cols[0])\n",
    "            if cols[1].startswith(' continuous.'):\n",
    "                types.append(float)\n",
    "            else:\n",
    "                types.append(str)\n",
    "    return names, types\n",
    "\n",
    "col_names, col_types = read_names('adult.names')\n",
    "\n",
    "def read_data(filename, col_names, col_types):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            cols = l.strip('\\n.').split(', ')\n",
    "            if len(cols) < len(col_names):\n",
    "                continue\n",
    "            X.append( { n:t(c) for n, t, c in zip(col_names, col_types, cols) } )\n",
    "            Y.append(cols[-1])\n",
    "    return X, Y\n",
    "\n",
    "# read the training set\n",
    "Xtrain, Ytrain = read_data('adult.data', col_names, col_types)\n",
    "\n",
    "# read the test set\n",
    "Xtest, Ytest = read_data('adult.test', col_names, col_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exemplify the instances in this dataset, let's print the input and output for the first instance. The input consists of a feature dictionary, containing named attributes such as `age`, `education` etc. The output is a string: in this case, either `'<=50K'` (low earner) or `'>50K'` (high earner). This is a *binary* classification problem because we have two output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 39.0,\n",
      " 'capital-gain': 2174.0,\n",
      " 'capital-loss': 0.0,\n",
      " 'education': 'Bachelors',\n",
      " 'education-num': 13.0,\n",
      " 'fnlwgt': 77516.0,\n",
      " 'hours-per-week': 40.0,\n",
      " 'marital-status': 'Never-married',\n",
      " 'native-country': 'United-States',\n",
      " 'occupation': 'Adm-clerical',\n",
      " 'race': 'White',\n",
      " 'relationship': 'Not-in-family',\n",
      " 'sex': 'Male',\n",
      " 'workclass': 'State-gov'}\n",
      "\n",
      "<=50K\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(Xtrain[0])\n",
    "print()\n",
    "print(Ytrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assemble the building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# The DictVectorizer is used to map symbolic features to numerical vectors.\n",
    "# Note that we set sparse=False, because our Perceptron implementation assumes\n",
    "# that the examples are stored in a NumPy matrix.\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# A StandardScaler divides the features by their standard deviation. The purpuse is that\n",
    "# the numerical features should have a similar magnitude.\n",
    "sc = StandardScaler(with_mean=False)\n",
    "\n",
    "# Make an instance of the perceptron class we implemented above.\n",
    "clf = Perceptron()\n",
    "\n",
    "# Combine the vectorizer, scaler and the classifier into a pipeline.\n",
    "pipeline = make_pipeline( dv, sc, clf )\n",
    "\n",
    "# Train the classifier, evaluate on the test set.\n",
    "pipeline.fit(Xtrain, Ytrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally run the classifier on the test set and compute its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7913956978489245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yguess = pipeline.predict(Xtest)\n",
    "accuracy_score(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the features learned by the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at what the perceptron algorithm has come up with. First, let's see which category corresponds to the positive scores, and which to the negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<=50K', '>50K')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.positive_class, clf.negative_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that positive scores will be interpreted as the category `<=50K`, and negative scores as `>50K`.\n",
    "\n",
    "We will then see which features the learning algorithm has assigned high weights to. We can first just look at the weights stored in the weight vector `w`, that we built in the `fit` method that we created previously. This is a long vector, so we'll just print the first 10 dimensions. We see that the first three features have negative weights, which shows that an increase in these features will increase our certainty that this person is a high earner (negative classifier score). The other seven features point in the other direction: increasing them makes the classifier think that this person is a low earner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -19.97589562, -101.36763558,  -24.22800453,   14.09665603,\n",
       "         24.10751822,   14.57480801,   10.39962876,    0.        ,\n",
       "         10.39962876,   22.16560518])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.w[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above didn't tell us that much, really, because it's not obvious how to interpret the positions. To understand the meaning of each position, we need to look into the `DictVectorizer` that we used to map named features into a feature matrix.\n",
    "\n",
    "In a `DictVectorizer`, this information is stored in the attribute called `feature_names_`. The feature names appear in the same order as they do in the weight vector. So this means that the first column in the feature matrix is `age`. The second feature, `capital-gain`, has a much stronger association with the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'education-num',\n",
       " 'education=10th',\n",
       " 'education=11th',\n",
       " 'education=12th',\n",
       " 'education=1st-4th',\n",
       " 'education=5th-6th',\n",
       " 'education=7th-8th']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.feature_names_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the 20 features that have the highest negative weights. In this case, the negative class is `>50K`, or the people who earned more than $50,000 a year. As you can see, features look quite meaningful: for instance, people who own capital or have a college degree are more likely to have a high income.\n",
    "\n",
    "(If you wonder about the functions `sorted` and `zip`, please take a look at the [documentation of Python built-in functions](https://docs.python.org/3/library/functions.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-gain -101.36763557732267\n",
      "native-country=Cambodia -38.7427498651224\n",
      "native-country=Italy -36.569732969241024\n",
      "marital-status=Married-civ-spouse -34.13648500928391\n",
      "education=Masters -25.951043559299272\n",
      "capital-loss -24.22800452508256\n",
      "occupation=Tech-support -23.448415270421496\n",
      "native-country=Iran -22.38307405179305\n",
      "native-country=Jamaica -22.383074051792764\n",
      "age -19.975895615583973\n",
      "native-country=England -17.34944795898751\n",
      "hours-per-week -16.194567820428155\n",
      "occupation=Exec-managerial -15.277167174810899\n",
      "native-country=Canada -13.729725486341648\n",
      "relationship=Not-in-family -13.53588551764624\n",
      "occupation=Prof-specialty -12.207389010415373\n",
      "education=Assoc-acdm -11.600057330203043\n",
      "fnlwgt -10.122362093930322\n",
      "education=Doctorate -10.050378152592227\n",
      "relationship=Wife -9.00791242218654\n"
     ]
    }
   ],
   "source": [
    "for weight, fname in sorted( zip(clf.w, dv.feature_names_) )[:20]:\n",
    "    print(fname, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, the features most strongly associated with the positive class (`<=50K`, low earners) also tend to be meaningful, such as being unemployed or not having an education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native-country=Laos 54.7813867426214\n",
      "native-country=Ecuador 54.7813867426214\n",
      "native-country=Yugoslavia 54.78138674262097\n",
      "native-country=Peru 54.78138674262073\n",
      "native-country=Scotland 54.78138674262064\n",
      "relationship=Own-child 53.45636183421069\n",
      "occupation=Armed-Forces 38.742749865123166\n",
      "native-country=Thailand 38.742749865123\n",
      "native-country=Trinadad&Tobago 38.74274986512226\n",
      "native-country=Vietnam 38.74274986512215\n",
      "marital-status=Never-married 34.088785554798896\n",
      "native-country=Columbia 31.63859985841679\n",
      "marital-status=Married-AF-spouse 31.638599858416644\n",
      "occupation=Farming-fishing 30.30804031055438\n",
      "sex=Female 29.922553484768287\n",
      "native-country=Nicaragua 27.404403571569752\n",
      "marital-status=Married-spouse-absent 25.23960880504893\n",
      "native-country=South 24.515335393364108\n",
      "native-country=Haiti 24.515335393363735\n",
      "education=9th 24.41509104406004\n"
     ]
    }
   ],
   "source": [
    "for weight, fname in sorted( zip(clf.w, dv.feature_names_), reverse=True)[:20]:\n",
    "    print(fname, weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
